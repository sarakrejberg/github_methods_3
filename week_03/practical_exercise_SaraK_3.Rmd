---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: '[Sara Krejberg]'
date: "[5/10 - 2021]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readbulk, patchwork, rstanarm, MuMIn, lme4, lmerTest, dfoptim)

```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame  
```{r}
experiment2 <- read_bulk('experiment_2')
```

2) Describe the data and construct extra variables from the existing variables  
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
```{r}
experiment2 <- experiment2 %>% 
  mutate(correct = ifelse(obj.resp == 'o' & target.type == 'odd' | obj.resp == 'e' & target.type == 'even', "1", "0")) 
```
    
    ii. describe what the following variables in the data frame contain:
Trial.type: consist of staircase and experiment. Staircase is the control condition while experiment is the experimental condition. 
Pas: Pas stands for the perceptual awareness scale. Goes from 1-4. The participant answers how they experienced the stimuli. 1, is no impression of the stimuli. 2, a feeling that something has been shown, 3:almost clear experience, 4: clear experience 
Trial: Number of trials for one participant. 
Target.contrast: The contrast of the screen compared to the stumuli. 
Cue: In each trial a cue is first presented, the cue indicated how many digits the participant could expect to see. The cur is repeated 12 times, and then changed.  
Task: The task can be either a single, pair or quadruplet. This is telling what task the participant is going to be shown. 
Target_type: This tells if the digit showed on the screen is even or odd. 
Rt.obj: This is the reaction time for the participant when answering if the number/target were even or odd number 
Rt.subj: Here the reaction time was added for when the participant answered the ubjective response, so answering on the PAS going from 1 - 4.
Obj.resp: Showed if the participant responded that the digit were even or odd.
Subject: The participant in the study 
Correct: Whether the participant answered correct or not.

    For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  

Correct should be changed into a factor because its only 2 different options and they are not dependent on each other. 
The subject should as well be changed to a factor because the different subjects are not depended on each other.
    
```{r}
# giving things correct class
experiment2$correct <- as.factor(experiment2$correct) #correct should actually be logical 
#experiment2$correct <- as.logical(experiment2$correct)
experiment2$subject <- as.factor(experiment2$subject)

#examining the dataframe and seeing its classes. 
glimpse(experiment2)

```
    
    iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions?
```{r}
staircase <- experiment2 %>% 
  filter(trial.type == 'staircase')

stairmodel <- glm(correct ~ target.contrast*subject, data = staircase, family = binomial(link = 'logit'))
summary(stairmodel)

stair.fitted <- fitted.values(stairmodel)

staircase%>% 
  ggplot(aes(x = target.contrast, y= stair.fitted))+
  geom_point()+
  facet_wrap(~subject)
 
```
    
    iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_  
```{r}
summary(staircase)

stairmodel1 <- glmer(correct ~ target.contrast + (1 + target.contrast | subject), data = staircase, family = binomial(link = 'logit'))

fit <- fitted(stairmodel1)

staircase%>% 
  ggplot(aes(target.contrast, stair.fitted))+
  geom_point()+
  geom_point(aes(y = fit, col = 'blue'))+
  facet_wrap(~subject)
```
    
    v. in your own words, describe how the partial pooling model allows for a better fit for each subject 
It allows for a better fit because the model takes into count that the subjects all have different beginning points. 

## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled  
```{r}
#only looking at experiment 
experiment <- experiment2 %>% 
  filter(trial.type == 'experiment')


#Doing one subject at a time 
subject1 <- experiment %>% 
  subset(subject == "1") 
subject2 <- experiment %>% 
  subset(subject == "2")
subject3 <- experiment %>% 
  subset(subject == "3")
subject4 <- experiment %>% 
  subset(subject == "4")


submodel1 <- lm(rt.obj ~ 1, data = subject1)
submodel2 <- lm(rt.obj ~ 1, data = subject2)
submodel3 <- lm(rt.obj ~ 1, data = subject3)
submodel4 <- lm(rt.obj ~ 1, data = subject4)

r1 <- resid(submodel1)
r2 <- resid(submodel2)
r3 <- resid(submodel3)
r4 <- resid(submodel4)
 
#plotting with qqplot
qqnorm(resid(submodel1))
qqline(resid(submodel1), col = 'lightblue')

qqnorm(resid(submodel2))
qqline(resid(submodel2), col = 'lightgreen')

qqnorm(resid(submodel3))
qqline(resid(submodel3), col = 'red')

qqnorm(resid(submodel4))
qqline(resid(submodel4), col = 'yellow')


  
```

    i. comment on these   
So looking at the qqplot we see that the plots are all right skewed. Therefore we can see that the residuals are not normally distributed. 

    ii. does a log-transformation of the response time data improve the Q-Q-plots? 
```{r}
#creating models 
submodel1.1 <- lm(log(rt.obj) ~ 1, data = subject1)
submodel2.1 <- lm(log(rt.obj) ~ 1, data = subject2)
submodel3.1 <- lm(log(rt.obj) ~ 1, data = subject3)
submodel4.1 <- lm(log(rt.obj) ~ 1, data = subject4)

#creating plots 
qqnorm(resid(submodel1.1))
qqline(resid(submodel1.1), col = 'lightblue')

qqnorm(resid(submodel2.1))
qqline(resid(submodel2.1), col = 'lightgreen')

qqnorm(resid(submodel3.1))
qqline(resid(submodel3.1), col = 'red')

qqnorm(resid(submodel4.1))
qqline(resid(submodel4.1), col = 'yellow')

```
Log-transforming the reactiontime improved the qq-plots. They are still not perfect, but way better than before.  

2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)  

    i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  
I am including subject as a random effect, to look at the  difference between subjects.
```{r}
model1 <- lmer(rt.obj ~ task + (1 | subject), data = experiment, REML = FALSE)

r.squaredGLMM(model1)

```
The model does not explain the variance. But putting in more random effects dosnet improve it but rather it may be overfitted. Therefore i stick with the simple model eventhough it dosnt explain much. 
  
    ii. explain in your own words what your chosen models says about response times between the different tasks  
It says that response time becomes slower when the task is pair compared to the two other tasks. In the single task participant performence the fastes reaction time.   

```{r}
summary(model1)
```

3) Now add _pas_ and its interaction with _task_ to the fixed effects 
```{r}
pasmodel <- lmer(rt.obj ~ task * pas + (1 | subject), data = experiment, REML = FALSE)
```

    i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits? 
```{r}
#when running this model i get the error singular fits
pasmodel1 <- lmerTest::lmer(rt.obj ~ task*pas + (1|subject) + (1|pas) + (1|trial) + (1|task), data = experiment, REML = FALSE)
#removing task as an random effect i get an error of singular fits.
pasmodel2 <- lmerTest::lmer(rt.obj ~ task*pas + (1|subject) + (1|pas) + (1|trial), data = experiment, REML = FALSE)
#removing pas as random effect and now getting a model without the error.
pasmodel3 <- lmerTest::lmer(rt.obj ~ task*pas + (1|subject) + (1|trial), data = experiment, REML = FALSE)

print(VarCorr(pasmodel1), comp='Variance')
#its shown that task explains 0 of the variance

print(VarCorr(pasmodel2), comp='Variance')
#Pas  also explains 0 of the variance and therefore it makes and error 

print(VarCorr(pasmodel3), comp='Variance')

```
    
    ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
    iii. in your own words - how could you explain why your model would result in a singular fit? 
Because of overfitting putting in task and pas does not explain any of the variance. Therefore i am in the risk of overfitting my model.  
    
## Exercise 3

1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r}
data.count <- experiment2 %>% 
  group_by(subject, pas, task) %>% 
  summarise('count' = n())

data.count$pas <- as.factor(data.count$pas)
data.count$task <- as.factor(data.count$task)
data.count$subject <- as.factor(data.count$subject)

```        

2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  
```{r}
model2 <- glmer(count ~ pas * task + (pas| subject), data = data.count, control = glmerControl(optimizer="bobyqa"), family = poisson)
```

    i. which family should be used?  
Poisson should be used because its the best when measuring positive natural numbers like count. 

    ii. why is a slope for _pas_ not really being modelled?  
Pas is a factor therefore there are no relationship between the different numbers in pas. This makes it not possible for modelling a slope, because the slope needs to be continues, and thereby dependent on the former number. 

    iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
    iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
```{r}
model3 <- glmer(count ~ pas + task + (pas| subject), data = data.count, family = poisson, control = glmerControl(optimizer="bobyqa"))

#measuring best fit 
AIC(model2, model3)
r.squaredGLMM(model2)
r.squaredGLMM(model3)

#Residuals Variance 
resid2 <- residuals(model2)
resid3 <- residuals(model3)
var2 <- sum(resid7^2)
var3 <- sum(resid8^2)
var2
var3
```

    v. indicate which of the two models, you would choose and why  
Model 7 which have the interaction. Its has the lowest value looking at AIC, and has the highest R2 value so it explains more variance than model 8. It also has the lowest variance. 

    vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  
The model show us that there are less ratings in the PAS 2,3 and 4. Thereby most participants have not seen the stimuli in many cases. This is though not the case with the interaction between task and pas here when its the single task we see an increase in the numbers indicating that participants were better at seeing the stimuli in the single task. 
```{r}
summary(model2)
```


    vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing 
```{r}
library(ggpubr)

#making a barplot showing the distribution of answers in the PAS 
#choosing subject
subject1 <- data.count %>% filter(subject == "1")
subject2 <- data.count %>% filter(subject == "2")
subject3 <- data.count %>% filter(subject == "3")
subject4 <- data.count %>% filter(subject == "4")

#the estimated ratings
subject1$predict <- predict(model2, newdata = subject1)
subject2$predict <- predict(model2, newdata = subject2)
subject3$predict <- predict(model2, newdata = subject3)
subject4$predict <- predict(model2, newdata = subject4)


#Plotting ratings for each subject 
subject.plot1 <- subject1 %>% 
  ggplot()+
  geom_bar(aes(pas, predict), stat = "identity",)+
  theme_bw() +
  ggtitle("Subject 1") +
  xlab("PAS")+
  ylab("Estimated count")

subject.plot2 <- subject2 %>% 
  ggplot()+
  geom_bar(aes(pas, predict), stat = "identity")+
  theme_bw() + 
  ggtitle("Subject 2")+
  xlab("PAS")+
  ylab("Estimated count")

subject.plot3 <- subject3 %>% 
  ggplot()+
  geom_bar(aes(pas, predict), stat = "identity")+
  theme_bw() + 
  ggtitle("Subject 3")+
  xlab("PAS")+
  ylab("Estimated count")

subject.plot4 <- subject4 %>% 
  ggplot()+
  geom_bar(aes(pas, predict), stat = "identity")+
  theme_bw()+
  ggtitle("Subject 4")+
  xlab("PAS")+
  ylab("Estimated count")

ggarrange(subject.plot1, subject.plot2, subject.plot3, subject.plot4)

#Making another visulasation. showing the number of counts in the different PAS in the three tasks  
ggplot(data.count, aes(x = pas, y = count)) +
  geom_point(aes(pas, count), color = "pink") +
  facet_wrap(~ task) +
  theme_bw()

#Ending with a boxplot showing the same thing
bp <- ggplot(data.count, aes(x=pas, y=count, group=pas)) + 
  geom_boxplot(aes(fill=pas))

bp + facet_grid(task ~ .)
```

3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  
```{r}
experiment2$task <- as.factor(experiment2$task)
experiment2$correct <- as.factor(experiment2$correct)
experiment2$pas <- as.factor(experiment2$pas)

model4 <- glmer(correct ~ task + (1 | subject), family = binomial, data = experiment2)

```

    i. does _task_ explain performance? 
```{r}
#For understanding the numbers which is at the logit scale we use the inverse logit scale so it comes out in propobilites instead. 
pair <- invlogit(1.10071)
quar <- invlogit(1.10071-0.09825)
single <- invlogit(1.10071+0.18542 )

pair
quar
single 
```
Task explains performance significantly. looking at the different  
Single: 78% for answering corrct in the single task 
pair: 75% for answering correct in the pair task 
Quar: 73% for answering correct in the quardroplet task. 
But they are pretty close to eachother.. 
 

    ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
```{r}
model5 <- glmer(correct ~ task + pas + (1 | subject), family = binomial, data = experiment2)
summary(model5)
```
The consequences - no significance in the task as there were in the other model. This might indicate that PAS is a better measure for answering correct than task.

    iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
```{r}
model6 <- glmer(correct ~ pas + (1 | subject), family = binomial, data = experiment2)
```

    iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
```{r}
model7 <- glmer(correct ~ task * pas + (1 | subject), family = binomial, data = experiment2)
```
    
    v. describe in your words which model is the best in explaining the variance in accuracy  
```{r}
AIC(model4, model5,model6, model7)

#R2
r.squaredGLMM(model4)
r.squaredGLMM(model5)
r.squaredGLMM(model6)
r.squaredGLMM(model7)

#residual variance 
resid4 <- residuals(model4)
resid5 <- residuals(model5)
resid6 <- residuals(model6)
resid7 <- residuals(model7)

var4 <- sum(resid9^2)
var5 <- sum(resid10^2)
var6 <- sum(resid11^2)
var7 <- sum(resid12^2)

var4
var5
var6
var7
```
Model 7 glmer(correct ~ task * pas + (1 | subject), family = binomial, data = experiment2) is the best at explaining variance. They are all pretty close to each other especially model 6 and 7. But the model where predicting accuracy by PAS and in the interaction with task is doing the best. It has the lowest variance residuals and the highest R2, so it explains the most variance. Looking at the AIC model 11 do the best, this is because its a more simple model. 
